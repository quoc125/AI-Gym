{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Algothrim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three search algothrims. Depth First search where, the first value in is the last value out. Breath First search is where the first value in is the first value out. Uniform cost search is you modify the breath first search to account for the cost. The lower the cost, it is going to be on top of the queue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A* search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A* use a two part algothrim. It use heurstics to calcuate the cost to get to the goal. The other part is to use cost algothrim to store the cost to get to that point. This improve the speed the algothrim and performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflex Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reflex agent allows pacman to finish the game by stay as far as possible from ghost, but tries to get as close as possible to the dot. This only works against a sub-optimial opponents. A optimial opponents would corner pacman. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimax agent continue with the concept learn in reflex agent by allows to calcuate the multiple different action that the user can peform and the opponents. The number of depth determine how actions it plans for. The min layer minimize the distance to the dot while the max maximumize the distance between the ghost. This assume that the oopponents is playing optimally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha-Beta pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha-Beta pruning improve the speed of minimax. Instead of looking at each value of the layer, it prune the each layer to find the optimal value. Alpha looks for the best max action, while beta looks for the best min action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectimax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectimax works better against a sub-optimal opponents as compared to minimax and alpha-beta pruning. It takes the average of each nodes based on the expected value based on the probability given. The result of the expectimax is depended on the model given. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov Decision Process development a model of the world on the agents training. It tries to careate an optimal policy that maximizes the expected utility. The reward that the agent takes is depended on the reward given during the markov decision processes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Evaluation calcuate the value of each iteration of training the MDP. It looks at the reward for completing each action, and look decay of staying alive. Than, it applies the tranistion function. IT takes the sumation of each result. It than use the max value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy Extraction looks at the result of policy evaluation and determine the best action to takes. It uses argmax with the policy evalutation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning is a reinforment learning algothrim that uses MDP to develop it's policy. Reinforcement learning allows the algothrim develop a policy by failure. The algothrim will eventually converge which allows it to develop an optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross entropy is a machine learning algothrim that trains a model based on a reward. It looks at the top x% that preform and mix up the action. It uses a one-hot encoding method so only one action is preform. The final model will peform an action depending on it's state. The action it's peform is based on it's training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
